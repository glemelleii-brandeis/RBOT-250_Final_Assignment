Gazebo - How do you put the pieces together?

I want to open with the fact that I've been using blender and have been working with 3D modeling programs for several years. I understand that in code, a model is usually defined by its point of origin, various parameters that define the coordinates of a geometric shape, and the position, rotation and scale factors which would adjust the object either relative to its origin or relative to the origin of the world. Thus in theory, you can combine that data with other physical data assigned to the objects to simulate the actions of a real world object in virtual space and get a feel for the math required to have a machine move in the same way as the model or expectation. The nice thing about Gazebo is as a software, it has the parameters set to take any objects you create, define them by their shape, define alternate collision based on new shapes, and also define the physics involved with the shape. A major pain point was the tutorials are not very user friendly as it took many attempts and lots of searching to find out how to write a program that could move a joint. Most of the tutorials talked about how to move static shapes around and automate this motion. Building a multi arm robot with no plugin will cause the robot to collapse on itself, and a simple moving plugin will cause the arm to stand stiff and shift, based on the tutorials on the site, but there is very little information on how to actually get the joints to move without a lot of set up and using ROS, and most of the tutorials mention this but link to pre-made packages that do everything behind the scenes. Thus it will take more time or effort to really come to grips with how Gazebo would enable these motions. 


In lieu of being successful at programming a robot and understanding how to get the robot to move to a particular point (due to time constraints), I can at least focus on describing the actions that were planned for this robot. Inverse Kinematics was planned to be used where the end effector of the robot would determine the spot that the arm would reach towards and with the right weights applied to the arm, the inertia of the rotary joints connecting the links together should prevent the robot from toppling on itself. As for the Inverse Kinematics. Based on my research, there were two forms of IK that could be applied to the robot arm depending on which method made more sense for the final build, Cyclic Coordinate Descent, and Forward and Backward Reaching Inverse Kinematics. The CCD method involves using rotations to determine the final position of the end effector by iterating through each joint's rotation and lining up each link with the goal. After a cycle completes, the cycle will repeat with the joints rotating again to inch its way closer to the goal, making adjustments for each rotary joint until the end effector is at the goal or all links are forming a straight line to the position of the goal or the end effector is as physically close to the goal as possible given the solid lengths of each arm. Each of these factors would have to be accounted for depending on the design of the arm. The FABRIK solver on the other hand uses position to determine how the arms should line up. It will change the position of each link so that the end effector is placed on the goal and the link is rotated to line up with the tip of the parent link. This repeats until all the links have been connected, then the chain will return to the base. The CCD design would make more sense for a physical bot solving in real time as the FABRIK solver would have to be resolved virtually, thus creating a virtual grid to determine the "positions" for the joints to reach and coming to the final conclusion before the robot can shift its arm. Plus the final rotations of the joints would have to be determined before the Robot moves. This can be more complicated to resolve as a result and more difficult to test on a physial machine due to physical limitations of the robot's movement.

I think the experience of setting up a model gives a little insight on how to create simulations and with a lot more time, I could see myself creating custom models in blender and putting these models together to create a more dynamic robot. There are many steps to keep track of for the simpler motions, but a large part of the difficulty comes from understanding ROS, C++/Python, and how to apply the logic of the code to the action of the robot, particularly since the file formats are incompatable across the different parts of the project and conversions have to be made (URDF vs SDF). Gazebo is able to run logic against an SDF by creating plugins but ROS requires URDFs to be able to control the robot and manipulate them directly. Thus Gazebo is able to apply simple motions OR you can export Collade data (which could be done in Blender for example) and import the Collade data to an SDF to generate more dynamic, albeit non-physics based animations. It is likely that the combination of the physics and non-physics based animation is what would lead to seeing actions like an arm being able to pick up an object and carry it since the actions of the machine would have to be pre-emptively programmed in anyway, but it makes sense when applying that to a physical machine as the robot arm would be controlled by the motors that would push the parts of the robot to certain locations. As long as the motion is physically possible, any physics simulations would be a matter of ensuring the motions make sense with the environment or use case, the collision data is correct, and the final motion data can be accurately reflected by the physical joints and constraints. 
